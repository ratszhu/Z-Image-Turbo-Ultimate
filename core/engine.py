# -*- coding: utf-8 -*-
"""
æ¨ç†å¼•æ“
è´Ÿè´£æ¨¡å‹çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼šåŠ è½½ã€æ˜¾å­˜ä¼˜åŒ–é…ç½®ã€ä»¥åŠæœ€ç»ˆçš„ç”Ÿæˆæ‰§è¡Œã€‚
"""
import torch
from diffusers import DiffusionPipeline # type: ignore
import gc
from core.utils import detect_device, get_torch_dtype
from core.lora_manager import LoRAMerger
import config

class ZImageEngine:
    def __init__(self):
        self.pipe = None
        self.device = None
        self.dtype = None
        self.lora_merger = None
        self.current_lora_applied = False

    def load_model(self):
        """
        åŠ è½½æ¨¡å‹ (è‡ªåŠ¨æ£€æµ‹è®¾å¤‡)ã€‚
        """
        # 1. è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
        self.device = detect_device()
        self.dtype = get_torch_dtype(self.device)
        
        print(f"ğŸš€ [Engine] æ­£åœ¨åŠ è½½æ¨¡å‹... è‡ªåŠ¨æ£€æµ‹è®¾å¤‡: {self.device.upper()}, ç²¾åº¦: {self.dtype}")
        
        # 2. æ¸…ç†æ—§æ˜¾å­˜ (é˜²æ­¢é‡è½½æ—¶çˆ†å†…å­˜)
        if self.pipe:
            del self.pipe
            self.pipe = None
            gc.collect()
            if torch.cuda.is_available(): torch.cuda.empty_cache()
            if torch.backends.mps.is_available(): torch.mps.empty_cache()

        # 3. åŠ è½½ Diffusers Pipeline
        try:
            self.pipe = DiffusionPipeline.from_pretrained(
                config.MODEL_PATH,
                torch_dtype=self.dtype,
                trust_remote_code=True,
            )
            self.pipe.to(self.device)
            
            # åˆå§‹åŒ– LoRA ç®¡ç†å™¨
            self.lora_merger = LoRAMerger(self.pipe)
            self.current_lora_applied = False
            
            # 4. åº”ç”¨ç¡¬ä»¶ç‰¹å®šçš„ä¼˜åŒ–ç­–ç•¥
            self._apply_optimizations()
            
            print("âœ… [Engine] æ¨¡å‹åŠ è½½å®Œæ¯•ï¼Œå‡†å¤‡å°±ç»ªã€‚")
            return f"å°±ç»ª | è®¾å¤‡: {self.device.upper()} | ç²¾åº¦: {self.dtype}"
            
        except Exception as e:
            print(f"âŒ [Engine] åŠ è½½å¤±è´¥: {e}")
            return f"åŠ è½½å¤±è´¥: {e}"

    def _apply_optimizations(self):
        """æ ¹æ®ç¡¬ä»¶ç±»å‹åº”ç”¨æ˜¾å­˜å’Œç”»è´¨ä¼˜åŒ–"""
        # [é€šç”¨] VAE ç²¾åº¦ä¿®å¤: å¼ºåˆ¶ FP32 ä»¥è§£å†³æ¨¡ç³Šé—®é¢˜
        if hasattr(self.pipe, "vae"):
            self.pipe.vae.to(dtype=torch.float32) # type: ignore
            self.pipe.vae.config.force_upcast = True # type: ignore
            print("ğŸ‘ï¸ [Optim] VAE å·²åˆ‡æ¢è‡³ FP32 (ç”»è´¨é”åŒ–)")

        # [Mac] M1/M2/M3 ä¼˜åŒ–
        if self.device == "mps":
            # å…³é—­ Tiling ä»¥è·å¾—æœ€ä½³æ¸…æ™°åº¦ (M1 Max æ˜¾å­˜è¶³å¤Ÿ)
            # å¦‚æœæ˜¯ 16G å†…å­˜çš„ Macï¼Œå¯èƒ½éœ€è¦å¼€å¯ self.pipe.enable_vae_tiling()
            print("ğŸ§  [Optim] MPS æ¨¡å¼: å·²é…ç½® Bfloat16 + VAE FP32ã€‚")
        
        # [Windows] NVIDIA ä¼˜åŒ–
        elif self.device == "cuda":
            # å¼€å¯ CPU Offload ä»¥èŠ‚çœæ˜¾å­˜ (è¿™å¯¹ 8G æ˜¾å­˜çš„ 4070 å¾ˆé‡è¦)
            self.pipe.enable_model_cpu_offload() # type: ignore
            if hasattr(self.pipe, "enable_vae_tiling"):
                self.pipe.enable_vae_tiling() # type: ignore
            print("ğŸ§  [Optim] CUDA æ¨¡å¼: CPU Offload å·²å¼€å¯ã€‚")

    def update_lora(self, enable, scale):
        """æ›´æ–° LoRA çŠ¶æ€ (å¯ç”¨/ç¦ç”¨/è°ƒæ•´å¼ºåº¦)"""
        # æƒ…å†µA: ä»æ— åˆ°æœ‰ -> ç›´æ¥åŠ è½½
        if enable and not self.current_lora_applied:
            self.lora_merger.load_lora_weights(config.LORA_PATH, scale) # type: ignore
            self.current_lora_applied = True
            return "LoRA å·²å¯ç”¨"
            
        # æƒ…å†µB: éœ€è¦å¸è½½æˆ–æ”¹å˜å‚æ•° -> é‡è½½æ¨¡å‹ (æœ€ç¨³å¦¥çš„æ–¹å¼)
        # å› ä¸ºæ‰‹åŠ¨æ³¨å…¥ä¿®æ”¹äº†æƒé‡ï¼Œä¸ºäº†ç”»è´¨çº¯å‡€ï¼Œæˆ‘ä»¬é€‰æ‹©é‡ç½®æ¨¡å‹
        if (not enable and self.current_lora_applied) or (enable and self.current_lora_applied):
            print("ğŸ”„ [Engine] LoRA è®¾ç½®å˜æ›´ï¼Œæ­£åœ¨é‡ç½®æ¨¡å‹...")
            self.load_model() # é‡è½½
            if enable:
                self.lora_merger.load_lora_weights(config.LORA_PATH, scale) # type: ignore
                self.current_lora_applied = True
            return "æ¨¡å‹å·²é‡ç½®å¹¶åº”ç”¨æ–° LoRA è®¾ç½®"

    def generate(self, prompt, neg_prompt, steps, cfg, width, height, seed, seed_mode):
        """ç”Ÿæˆå›¾ç‰‡çš„æ ¸å¿ƒé€»è¾‘"""
        # æ˜¾å­˜æ¸…ç†
        gc.collect()
        if self.device == "mps": torch.mps.empty_cache()
        if self.device == "cuda": torch.cuda.empty_cache()

        # ç§å­å¤„ç†é€»è¾‘
        if seed_mode == "éšæœº" or seed == -1:
            actual_seed = torch.randint(0, 2**32 - 1, (1,)).item()
        else:
            actual_seed = int(seed)
            
        # åˆ›å»º Generator (MPS éœ€è¦åœ¨ CPU åˆå§‹åŒ–)
        gen_device = "cpu" if self.device == "mps" else self.device
        generator = torch.Generator(gen_device).manual_seed(actual_seed) # type: ignore

        print(f"ğŸ¨ [Generate] å°ºå¯¸: {width}x{height} | æ­¥æ•°: {steps} | ç§å­: {actual_seed}")

        try:
            image = self.pipe(
                prompt=prompt,
                negative_prompt=neg_prompt,
                num_inference_steps=steps,
                guidance_scale=cfg,
                width=width,
                height=height,
                generator=generator
            ).images[0] # type: ignore
            
            return image, f"Used Seed: {actual_seed}"
        except Exception as e:
            return None, f"Error: {str(e)}"