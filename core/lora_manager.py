'''
Descripttion: 
Author: æœ±ä¸œå¸…
Date: 2025-12-08 13:45:44
LastEditors: æœ±ä¸œå¸…
LastEditTime: 2025-12-08 14:01:12
'''
# -*- coding: utf-8 -*-
"""
LoRA ç®¡ç†å™¨
ä¸“é—¨å¤„ç† Z-Image ç­‰éæ ‡å‡†ç»“æ„æ¨¡å‹çš„ LoRA æƒé‡æ³¨å…¥ã€‚
ç”±äº Z-Image åŒ…å«ç‰¹æ®Šçš„ Refiner å±‚ï¼Œæ™®é€šçš„ load_lora_weights æ— æ³•ç”Ÿæ•ˆï¼Œ
å¿…é¡»ä½¿ç”¨æ­¤ç±»è¿›è¡Œå±‚å¯¹å±‚çš„ç²¾å‡†æ³¨å…¥ã€‚
"""
import torch
import safetensors.torch
import re
import os

class LoRAMerger:
    def __init__(self, pipeline):
        self.pipeline = pipeline
        self.loaded_path = None

    def load_lora_weights(self, lora_path: str, lora_scale: float = 1.0):
        """åŠ è½½å¹¶åˆå¹¶ LoRA æƒé‡"""
        if not os.path.exists(lora_path):
            print(f"âš ï¸ [LoRA Manager] æ–‡ä»¶æœªæ‰¾åˆ°: {lora_path}ï¼Œè·³è¿‡åŠ è½½ã€‚")
            return

        print(f"ğŸ¨ [LoRA Manager] æ­£åœ¨æ³¨å…¥ LoRA: {os.path.basename(lora_path)} (å¼ºåº¦: {lora_scale})...")
        try:
            tensors = safetensors.torch.load_file(lora_path)
            self._merge_lora_weights(tensors, lora_scale)
            self.loaded_path = lora_path
        except Exception as e:
            print(f"âŒ [LoRA Manager] æ³¨å…¥å¤±è´¥: {e}")

    def _get_module_from_path(self, module_path):
        """å·¥å…·å‡½æ•°ï¼šé€šè¿‡å­—ç¬¦ä¸²è·¯å¾„è·å–æ¨¡å‹ä¸­çš„å±‚å¯¹è±¡"""
        try:
            parts = module_path.split('.')
            current = self.pipeline
            for part in parts:
                if part.isdigit():
                    current = current[int(part)]
                else:
                    current = getattr(current, part)
            return current
        except AttributeError:
            return None

    def _get_module_path_from_lora_key(self, lora_key):
        """æ ¸å¿ƒé€»è¾‘ï¼šå°† LoRA çš„é”®åæ˜ å°„åˆ° Z-Image æ¨¡å‹çš„å®é™…å±‚è·¯å¾„"""
        key = lora_key.replace('diffusion_model.', '')
        
        # 1. åŒ¹é… Refiner å±‚ (è¿™æ˜¯ Z-Image ç”»è´¨å¥½çš„å…³é”®)
        context_match = re.match(r'context_refiner\.(\d+)\.attention\.(to_q|to_k|to_v|to_out\.0)', key)
        if context_match:
            layer_idx, target = context_match.groups()
            return f"transformer.context_refiner.{layer_idx}.attention.{target}"

        noise_match = re.match(r'noise_refiner\.(\d+)\.attention\.(to_q|to_k|to_v|to_out\.0)', key)
        if noise_match:
            layer_idx, target = noise_match.groups()
            return f"transformer.noise_refiner.{layer_idx}.attention.{target}"

        # 2. åŒ¹é…å¸¸è§„ Transformer å±‚
        if key.startswith('layers.'):
            return f"transformer.{key}"
            
        return None

    def _merge_lora_weights(self, lora_state_dict, lora_scale):
        """æ‰§è¡Œæƒé‡åˆå¹¶ï¼šW_new = W_old + (B @ A) * scale"""
        count = 0
        device = self.pipeline.device
        
        for key in lora_state_dict.keys():
            if ".lora_A.weight" in key:
                base_key = key.replace(".lora_A.weight", "")
                b_key = f"{base_key}.lora_B.weight"
                alpha_key = f"{base_key}.alpha"
                
                module_path = self._get_module_path_from_lora_key(key)
                if not module_path: continue
                
                module = self._get_module_from_path(module_path)
                if module is None or not hasattr(module, 'weight'): continue

                # ä¸´æ—¶æå‡åˆ° FP32 è¿›è¡Œè®¡ç®—ä»¥ä¿è¯ç²¾åº¦
                A = lora_state_dict[key].to(dtype=torch.float32, device=device)
                B = lora_state_dict[b_key].to(dtype=torch.float32, device=device)
                
                rank = A.shape[0]
                alpha = lora_state_dict[alpha_key].item() if alpha_key in lora_state_dict else rank
                scale = lora_scale * (alpha / rank)
                
                # è®¡ç®—å¢é‡å¹¶æ³¨å…¥
                delta = (B @ A) * scale
                
                with torch.no_grad():
                    module.weight.data += delta.to(module.weight.dtype)
                    count += 1
                    
        print(f"âœ… [LoRA Manager] æ³¨å…¥å®Œæˆï¼Œå…±ä¿®æ”¹ {count} å±‚æƒé‡ã€‚")